---
title: 'Automatic DAG learning - part 1'
author: Iyar Lin
date: '2019-07-23'
slug: automatic_DAG_learning_part_1
categories:
  - R
tags: [R, simulation]
comments: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, cache = F)
set.seed(1)
options(scipen = 999)

packages <- c(
  "tidyverse", # best thing that ever happend to me
  "pander", # table rendering
  "bnlearn", # structure learning algorithms
  "carData", # GSSvocab dataset
  "ggdag", # ggplot DAGs
  "orientDAG", 
  "dagitty", # Create DAGs
  "SID", # structIntervDist
  "gridExtra", # arranging several ggplots together
  "doParallel", # parallel processing
  "foreach" # foreach loop
)

if (!require("pacman")) install.packages("pacman")
pacman::p_load(char = packages)
pacman::p_load_gh("IyarLin/simMixedDAG")
source("../miscellaneous files/utils_functions.R")
```

![](/post/sim_mixed_dag_with_simMixedDAG_files/simMixedDAGhex.png){width=380px height=400px}


# Intro 

At the end of my [first blog](link) post I've deferred discussion of DAG recovery to a later post. Well, that post has finally come!

There a few approaches I can think of to produce a model DAG for use in Causal Inference:

1. Use domain knowledge and theory  
1. Given a few candidate model DAGs one can perform statistical tests to compare their fit to the data at hand  
1. Use algorithms to automatically "learn" the underlying DAG  

In this post I'll explore the last approach. Being able to learn the underlying model DAG from a dataset enables almost automatic end-to-end Causal Inference in a very similar way to classic ML.

# Existing methods for learning DAGs

In this post I'll restrict the discussion to the scenarion where there are no hidden confounders (a strong, yet common asumption) and the dataset contains both discrete (i.e. categorical) and numeric variables.

The great [causalDisco page](https://annehpetersen.shinyapps.io/causalDisco/) tool enables getting the R functions that enable DAG estimation under the above specified conditions. 

We end up with the following list: *bnlearn::hc*, *bnlearn::tabu*, *bnlearn::mmhc*, *bnlearn::rsmax2*, *deal::autosearch* and *deal::heuristic*.

I've played a bit with the *deal* package functions and found the way they are used to be too manual for my needs. I've thus decided to use only the *bnlearn* functions.

# Estimated DAG accuracy measurements

Given a true DAG how can we measure the extent to which an estimated DAG is different from it?

## Edge difference

A simple way to measure estimated DAG accuracy would be to count how many edges are present in the true DAG yet missing in the estimated DAG and conversly, how many edges are present in the estimated DAG yet do not exist in the true DAG.

```{r}
g <- dagitty("dag {
ageGroup [pos=\"0,0\"]
vocab [pos=\"1,-1\"]
nativeBorn [pos=\"2,-2\"]
educ [pos=\"3,-1\"]
gender [pos=\"4,0\"]
nativeBorn -> educ
nativeBorn -> vocab
educ -> vocab
gender -> educ
ageGroup -> vocab
}")

tidy_dag <- tidy_dagitty(g)
tidy_dag$data$xend[1] <- 0.92
tidy_dag$data$yend[1] <- -0.96
tidy_dag$data$xend[2] <- 1.09
tidy_dag$data$xend[3] <- 3.08
tidy_dag$data$yend[3] <- -0.96
tidy_dag$data$xend[4] <- 2.92
tidy_dag$data$yend[4] <- -1.04
tidy_dag$data$xend[5] <- 1.07
tidy_dag$data$yend[5] <- -1.04

p1 <- ggdag(tidy_dag, node_size = 28) + theme_dag_blank() + ggtitle("True DAG") + 
  theme(plot.title = element_text(size = 30, hjust = 0.5)) + 
  scale_dag(expand_x = expand_scale(c(0.2, 0.2)), expand_y = expand_scale(c(0.2, 0.2)))

g2 <- dagitty("dag {
ageGroup [pos=\"0,0\"]
vocab [pos=\"1,-1\"]
nativeBorn [pos=\"2,-2\"]
educ [pos=\"3,-1\"]
gender [pos=\"4,0\"]
gender -> ageGroup
nativeBorn -> vocab
educ -> vocab
educ -> gender
ageGroup -> vocab
}")

tidy_dag2 <- tidy_dagitty(g2)
tidy_dag2$data$xend[1] <- 0.92
tidy_dag2$data$yend[1] <- -0.96
tidy_dag2$data$xend[2] <- 3.93
tidy_dag2$data$yend[2] <- -0.04
tidy_dag2$data$xend[3] <- 1.1
tidy_dag2$data$xend[4] <- 0.1
tidy_dag2$data$xend[5] <- 1.07
tidy_dag2$data$yend[5] <- -1.04

p2 <- ggdag(tidy_dag2, node_size = 28) + theme_dag_blank() + ggtitle("Estimated DAG") + 
  theme(plot.title = element_text(size = 30, hjust = 0.5)) + 
  scale_dag(expand_x = expand_scale(c(0.2, 0.2)), expand_y = expand_scale(c(0.2, 0.2)))
```

Let's take as an example the DAG presented on my [last post](link) presented on left, along with an estimated DAG estimated on the right:

```{r, fig.width=6, fig.height=6}
grid.arrange(p1, p2, nrow = 1)
```

We can see in the above example that in the estimated DAG there's an edge between *gender* and *ageGroup* which doesn't exist in the true DAG, and that it's missing the edge between *nativeBorn* and *educ*. The total edge distance would be in this case 2.

## Hamming distance

Even if an edge exists in 2 DAGs, it would make sense to penalize it if the edge orientation is wrong. The Hamming distance is the same as the edge distance, only it also counts edges oriented incorrectly. In the above example we can see that the edge between *educ* and *gender* is not oriented correctly and thus the Hamming distance in this case would be 3.

## Structural Intervention Distance

The reason we construct model DAGs to begin with is to enable correct adjustment set specification. It would thus make sense to measure DAG estimation accuracy by how well it serves the purpose of finding adjustment sets.

```{r}
sid <- structIntervDist(dagitty_to_adjmatrix(g), dagitty_to_adjmatrix(g2))
vars <- names(g)
wrong_sets <- data.frame(treatment = vars[which(sid$incorrectMat == 1, arr.ind = T)[, 1]], 
                         exposure = vars[which(sid$incorrectMat == 1, arr.ind = T)[, 2]]) %>% 
  mutate(
    `True set` = mapply(function(x,y) {
    as.character.adjustmentSets(adjustmentSets(g, x, y))}, 
    x = treatment, y = exposure
    ), 
    `Estimated set` = mapply(function(x,y) {
    as.character.adjustmentSets(adjustmentSets(g2, x, y))}, 
    x = treatment, y = exposure
    ))
```

The [Structural Intervention Distance](https://arxiv.org/abs/1306.1043) (SID) counts all possible treatment - exposure pairs where the estimated DAG does not yield a correct adjsutment set.

Below is a table giving the treatment - exposure pairs along with the true and estimated adjustment sets for the cases where they are different in our example above (we note an empty set {} means no conditioning is required):

```{r, results = "asis"}
pandoc.table(wrong_sets)
```

The SID in the example above would thus be `r sid$sid`.

# DAG estimation algorithms benchmarking

In this section we'll use the [simMixedDAG](https://github.com/IyarLin/simMixedDAG) package to simulate datasets from a DAG, and benchmark different algorithms on recovering it using the above measures.

The DAG and data generating process (DGP) in this section will be based on the [General Social Survey](https://vincentarelbundock.github.io/Rdatasets/doc/carData/GSSvocab.html) data. 

The data structure is shown below:

```{r, results = "asis"}
data("GSSvocab")
GSSvocab <- GSSvocab %>% filter(complete.cases(.)) %>% 
  mutate(year = as.numeric(as.character(year)))
structure_df(GSSvocab) %>% pandoc.table(caption = paste0(nrow(GSSvocab), " Observations"))
```

We'll assume the following DAG:

```{r}
set.seed(2)
true_dag <- as.matrix(SID::randomDAG(p = ncol(GSSvocab), probConnect = 0.3))
dimnames(true_dag) <- list(names(GSSvocab), names(GSSvocab))
true_dag_dagitty <- adjmatrix_to_dagitty(true_dag)
coordinates(true_dag_dagitty) <- list(
  x = setNames(object = c(-2.241, 0.466, -0.079, -5.233, 1.324, 0.366,
                          2.100, 3.258), 
               nm = c("age", "ageGroup", "educ", "educGroup", "gender",
                                    "nativeBorn", "vocab", "year")), 
  y = setNames(object = c(-0.416, -5.303, 1.955, -0.753, 3.763, -2.297,
                          0.460, 2.494), 
               nm = c("age", "ageGroup", "educ", "educGroup", "gender",
                                    "nativeBorn", "vocab", "year"))
)
ggdag(true_dag_dagitty, stylized = F, node_size = 20, text_size = c(5, 5, 2.9, 5, 2.9, 4, 5, 2.9)) + theme_dag_blank() + 
  theme(plot.title = element_text(size = 30, hjust = 0.5)) + 
  scale_dag(expand_x = expand_scale(c(0.2, 0.2)), expand_y = expand_scale(c(0.2, 0.2)))
```

The DGP will be constructed using the *non_parametric_dag_model* function.

# Orienting edges

After identifying the DAG skeleton (just the edges, without orientation) we can utilize methods that determine the causal direction between 2 variables in order to oriend the edges.

When the 2 variables are continuous, we can use the generalized correlation measure developed by [Vinod](http://dx.doi.org/10.1080/03610918.2015.1122048).

When the 2 variables are discrete, we can use distance correlation measures (see [Liu and Chan, 2016](https://arxiv.org/pdf/1803.07712.pdf)).

When one of the variables is discrete, and the other is continuous we can discretisize the continuous variable (using for example the procedure at infotheo::discretize) and use the method for 2 discrete variables.


```{r}
M <- 200
samples <-  c(seq(40, 120, by = 40)^2, nrow(GSSvocab), 80000)
M <- 50
samples <- 80000
ncores <- detectCores() - 1

results <- expand.grid(n = samples, 
                       M = 1:M, stringsAsFactors = F) %>% 
  mutate(tabu = NA, tabu_cont = NA, tabu_cont_disc = NA, tabu_cont_disc_discont = NA, tabu_full = NA, 
         core = sample(rep(seq(1, ncores - 1), length.out = n()), size = n(), replace = F)) %>% 
  nest(-core)

node_classes <- sapply(row.names(true_dag), function(node) class(GSSvocab[[node]]))

measure_DAG_accuracy <- function(df){
  for(i in 1:nrow(df)){
    sim_data <- simMixedDAG::sim_mixed_dag(non_param_dag_model, N = df$n[i])
    est_dag <- bnlearn::tabu(sim_data)
    est_dag <- orientDAG::bn_to_adjmatrix(est_dag)
    est_dag <- est_dag[match(rownames(true_dag), rownames(est_dag)), 
                       match(colnames(true_dag), colnames(est_dag))]
    df$tabu[i] <- orientDAG::dag_dist(true_dag, est_dag, distance_measure = "sid")
    est_dag2 <- orientDAG::orient_dag(adjmatrix = est_dag, 
                                      x = sim_data)
    df$tabu_full[i] <- orientDAG::dag_dist(true_dag, est_dag2, distance_measure = "sid")
    
    if(sum(est_dag[node_classes == "numeric", node_classes == "numeric"]) > 0){
      est_dag[node_classes == "numeric", node_classes == "numeric"] <- 
        orientDAG::orient_dag(adjmatrix = est_dag[node_classes == "numeric", node_classes == "numeric"], 
                              x = sim_data)
      df$tabu_cont[i] <- orientDAG::dag_dist(true_dag, est_dag, distance_measure = "sid")
    }
    
    if(sum(est_dag[node_classes == "factor", node_classes == "factor"]) > 0){
      est_dag[node_classes == "factor", node_classes == "factor"] <- 
        orientDAG::orient_dag(adjmatrix = est_dag[node_classes == "factor", node_classes == "factor"], 
                              x = sim_data)
      df$tabu_cont_disc[i] <- orientDAG::dag_dist(true_dag, est_dag, distance_measure = "sid")
    }
    
    pairs <- which(est_dag == 1, arr.ind = T)
    pairs_classes <- apply(pairs, 2, function(x) node_classes[x])
    discont_pairs <- pairs[pairs_classes[, 1] != pairs_classes[, 2], ]
    if(is.matrix(discont_pairs)){
      discont_mat <- matrix(0, ncol = ncol(est_dag), nrow = nrow(est_dag), dimnames = dimnames(est_dag))
      discont_mat[discont_pairs] <- 1
      discont_mat <- orientDAG::orient_dag(adjmatrix = discont_mat, x = sim_data)
      est_dag[rbind(discont_pairs, cbind(discont_pairs[, 2], discont_pairs[, 1]))] <- 
        discont_mat[rbind(discont_pairs, cbind(discont_pairs[, 2], discont_pairs[, 1]))]
      df$tabu_cont_disc_discont[i] <- orientDAG::dag_dist(true_dag, est_dag, distance_measure = "sid")
    }
  }
  return(df)
}

if(!"dag_accuracy2.rds" %in% list.files("../../")){
  non_param_dag_model <- non_parametric_dag_model(true_dag_dagitty, GSSvocab)
  cl <- makeCluster(ncores)
  registerDoParallel(cl)
  dag_accuracy2 <- foreach (i=1:nrow(results), .combine = rbind) %dopar% {
    measure_DAG_accuracy(results$data[[i]])
  }
  stopCluster(cl)
  saveRDS(dag_accuracy2, "../../dag_accuracy2.rds")
} else {
  dag_accuracy2 <- readRDS("../../dag_accuracy2.rds")
}
```

The results above indicate that the *bnlearn::tabu* function has the best performance. We'll build a new function called "dag_learn" which after using *bnlearn::tabu* to learn the skeleton, adds another step of orienting the edges.

```{r}
dag_accuracy2 %>% select(-c(M)) %>% gather(key = algorithm, value = sid, -n) %>% 
  filter(!is.na(sid)) %>% group_by(n, algorithm) %>% 
  summarise(lower = quantile(sid, 0.05), upper = quantile(sid, 0.95), sid = mean(sid)) %>% 
  ggplot(aes(n, sid, color = algorithm)) + geom_point(size = 2) + scale_x_continuous(breaks = samples) + 
  geom_errorbar(aes(ymin = lower, ymax = upper), alpha = 0.2) + theme_bw() + theme(panel.grid = element_blank())
```



```{r}
nodes <- length(names(true_dag_dagitty))
dag_accuracy2 %>% 
  group_by(algorithm, n) %>% 
  summarise(lower = quantile(sid, 0.05), upper = quantile(sid, 0.95), sid = mean(sid)) %>% 
  ggplot(aes(n, sid, color = algorithm)) + geom_point(size = 2) + scale_x_continuous(breaks = samples) + 
  geom_errorbar(aes(ymin = lower, ymax = upper), alpha = 0.2) + theme_bw() + theme(panel.grid = element_blank())
```

```{r}
set.seed(1)
sim_data <- sim_mixed_dag(non_param_dag_model, N = 80000)
est_dag <- tabu(sim_data)
if(class(est_dag) == "bn") est_dag <- bn_to_adjmatrix(est_dag)
est_dag <- est_dag[match(rownames(true_dag), rownames(est_dag)), 
                   match(colnames(true_dag), colnames(est_dag))]

tabu_sid <- structIntervDist(true_dag, est_dag)$sid

tabu_dag_dagitty <- adjmatrix_to_dagitty(est_dag)
coordinates(tabu_dag_dagitty) <- list(
  x = setNames(object = c(-2.241, 0.466, -0.079, -5.233, 1.324, 0.366,
                          2.100, 3.258), 
               nm = c("age", "ageGroup", "educ", "educGroup", "gender",
                                    "nativeBorn", "vocab", "year")), 
  y = setNames(object = c(-0.416, -5.303, 1.955, -0.753, 3.763, -2.297,
                          0.460, 2.494), 
               nm = c("age", "ageGroup", "educ", "educGroup", "gender",
                                    "nativeBorn", "vocab", "year"))
)

p1 <- ggdag(tabu_dag_dagitty, stylized = F, node_size = 20, text_size = c(5, 5, 2.9, 5, 2.9, 4, 5, 2.9)) + theme_dag_blank() + 
  theme(plot.title = element_text(size = 30, hjust = 0.5)) + 
  scale_dag(expand_x = expand_scale(c(0.2, 0.2)), expand_y = expand_scale(c(0.2, 0.2))) + 
  ggtitle("Tabu dag")

p2 <- ggdag(true_dag_dagitty, stylized = F, node_size = 20, text_size = c(5, 5, 2.9, 5, 2.9, 4, 5, 2.9)) + theme_dag_blank() + 
  theme(plot.title = element_text(size = 30, hjust = 0.5)) + 
  scale_dag(expand_x = expand_scale(c(0.2, 0.2)), expand_y = expand_scale(c(0.2, 0.2))) + 
  ggtitle("True dag")

grid.arrange(p2, p1, nrow = 1)
```


```{r}
est_dag <- dag_learn(sim_data, orient_continuous = F)
if(class(est_dag) == "bn") est_dag <- bn_to_adjmatrix(est_dag)
est_dag <- est_dag[match(rownames(true_dag), rownames(est_dag)), 
                   match(colnames(true_dag), colnames(est_dag))]

dag_learn_sid <- structIntervDist(true_dag, est_dag)$sid

dag_learn_dag_dagitty <- adjmatrix_to_dagitty(est_dag)
coordinates(dag_learn_dag_dagitty) <- list(
  x = setNames(object = c(-2.241, 0.466, -0.079, -5.233, 1.324, 0.366,
                          2.100, 3.258), 
               nm = c("age", "ageGroup", "educ", "educGroup", "gender",
                                    "nativeBorn", "vocab", "year")), 
  y = setNames(object = c(-0.416, -5.303, 1.955, -0.753, 3.763, -2.297,
                          0.460, 2.494), 
               nm = c("age", "ageGroup", "educ", "educGroup", "gender",
                                    "nativeBorn", "vocab", "year"))
)

p3 <- ggdag(dag_learn_dag_dagitty, stylized = F, node_size = 20, text_size = c(5, 5, 2.9, 5, 2.9, 4, 5, 2.9)) + theme_dag_blank() + 
  theme(plot.title = element_text(size = 30, hjust = 0.5)) + 
  scale_dag(expand_x = expand_scale(c(0.2, 0.2)), expand_y = expand_scale(c(0.2, 0.2))) + 
  ggtitle("dag_learn dag")

grid.arrange(p2, p1, p3, nrow = 1)
```


